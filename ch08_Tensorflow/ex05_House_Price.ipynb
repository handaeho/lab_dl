{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House_Price.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fAnZBTatT0Q",
        "colab_type": "text"
      },
      "source": [
        "# housing.csv파일로 Boston House Price 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aHTGn0tnXFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjtEL7i7wr0f",
        "colab_type": "text"
      },
      "source": [
        "**업로드한 housing.csv 파일을 읽고 데이터 프레임 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud7GyAtCpvsK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "8990812e-5680-47a1-f224-245118e8b105"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('housing.csv', header=None, delim_whitespace=True)\n",
        "# ~> header=None: csv 파일에 컬럼 이름들이 없기 때문\n",
        "# ~> delim_whitespace=True: csv 파일의 데이터가 공백으로 구분되고 있기 때문\n",
        "df.head()\n",
        "# print(df.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1     2   3      4      5   ...  8      9     10      11    12    13\n",
              "0  0.00632  18.0  2.31   0  0.538  6.575  ...   1  296.0  15.3  396.90  4.98  24.0\n",
              "1  0.02731   0.0  7.07   0  0.469  6.421  ...   2  242.0  17.8  396.90  9.14  21.6\n",
              "2  0.02729   0.0  7.07   0  0.469  7.185  ...   2  242.0  17.8  392.83  4.03  34.7\n",
              "3  0.03237   0.0  2.18   0  0.458  6.998  ...   3  222.0  18.7  394.63  2.94  33.4\n",
              "4  0.06905   0.0  2.18   0  0.458  7.147  ...   3  222.0  18.7  396.90  5.33  36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZyJH6DRwxv2",
        "colab_type": "text"
      },
      "source": [
        "**DataFrame을 데이터(집값에 영향을 미치는 변수들)과 집값을 분리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEOonELXsv64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "133fa300-bcdc-4b05-bc5d-0721923158cc"
      },
      "source": [
        "dataset = df.to_numpy()\n",
        "X = dataset[:, :-1]\n",
        "Y = dataset[:, -1]\n",
        "print(f'X shape: {X.shape}, Y shape: {Y.shape}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (506, 13), Y shape: (506,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rxqhqLgw0hm",
        "colab_type": "text"
      },
      "source": [
        "**X, Y를 train_set / test_set으로 분할**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV7Oj_IVwMif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4d7f9611-ed62-4041-fca7-a6012156eb85"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
        "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')\n",
        "print(f'Y_train shape: {Y_train.shape}, Y_test shape: {Y_test.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (354, 13), X_test shape: (152, 13)\n",
            "Y_train shape: (354,), Y_test shape: (152,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7BUnlMTw7GO",
        "colab_type": "text"
      },
      "source": [
        "**신경망 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWslVQgtw-Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Fully-connected layer 추가 - hidden_layer 2개, output_layer\n",
        "model.add(Dense(30, activation='relu', input_dim=13)) # ~> 1st_hidden_layer\n",
        "# input_dim=X_trian.shape[1]\n",
        "\n",
        "model.add(Dense(6, activation='relu')) # ~> 2nd_hidden_layer\n",
        "\n",
        "model.add(Dense(1)) # ~> output_layer\n",
        "# 활성화 함수는 '예측 집값'이라는 output이 1개여서 없어도 된다. \n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='mean_squared_error', # 분류가 아닌 수치 예측이므로 MSE\n",
        "              optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "563WBaW8zNzg",
        "colab_type": "text"
      },
      "source": [
        "**모델 학습(fit)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgjaNo_XzP8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb7b973a-4f66-4d0d-8296-bbd5718610cf"
      },
      "source": [
        "history = model.fit(X_train, Y_train, batch_size=10, epochs=200,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 283 samples, validate on 71 samples\n",
            "Epoch 1/200\n",
            "283/283 [==============================] - 2s 8ms/sample - loss: 10363.4402 - val_loss: 1694.2714\n",
            "Epoch 2/200\n",
            "283/283 [==============================] - 0s 377us/sample - loss: 721.4364 - val_loss: 277.7735\n",
            "Epoch 3/200\n",
            "283/283 [==============================] - 0s 393us/sample - loss: 318.9620 - val_loss: 178.5265\n",
            "Epoch 4/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 245.2344 - val_loss: 147.2220\n",
            "Epoch 5/200\n",
            "283/283 [==============================] - 0s 360us/sample - loss: 215.5228 - val_loss: 151.1944\n",
            "Epoch 6/200\n",
            "283/283 [==============================] - 0s 390us/sample - loss: 201.6328 - val_loss: 123.6031\n",
            "Epoch 7/200\n",
            "283/283 [==============================] - 0s 385us/sample - loss: 174.1253 - val_loss: 118.5946\n",
            "Epoch 8/200\n",
            "283/283 [==============================] - 0s 374us/sample - loss: 163.5400 - val_loss: 114.3042\n",
            "Epoch 9/200\n",
            "283/283 [==============================] - 0s 333us/sample - loss: 154.2640 - val_loss: 114.1964\n",
            "Epoch 10/200\n",
            "283/283 [==============================] - 0s 342us/sample - loss: 149.0237 - val_loss: 104.8706\n",
            "Epoch 11/200\n",
            "283/283 [==============================] - 0s 299us/sample - loss: 139.7756 - val_loss: 99.7564\n",
            "Epoch 12/200\n",
            "283/283 [==============================] - 0s 301us/sample - loss: 136.6171 - val_loss: 110.3696\n",
            "Epoch 13/200\n",
            "283/283 [==============================] - 0s 424us/sample - loss: 132.7288 - val_loss: 94.7888\n",
            "Epoch 14/200\n",
            "283/283 [==============================] - 0s 368us/sample - loss: 123.3479 - val_loss: 85.4529\n",
            "Epoch 15/200\n",
            "283/283 [==============================] - 0s 353us/sample - loss: 117.3387 - val_loss: 82.0514\n",
            "Epoch 16/200\n",
            "283/283 [==============================] - 0s 387us/sample - loss: 109.5450 - val_loss: 74.9491\n",
            "Epoch 17/200\n",
            "283/283 [==============================] - 0s 334us/sample - loss: 107.1653 - val_loss: 77.4220\n",
            "Epoch 18/200\n",
            "283/283 [==============================] - 0s 375us/sample - loss: 100.6178 - val_loss: 72.2035\n",
            "Epoch 19/200\n",
            "283/283 [==============================] - 0s 348us/sample - loss: 97.8755 - val_loss: 64.7770\n",
            "Epoch 20/200\n",
            "283/283 [==============================] - 0s 337us/sample - loss: 95.1723 - val_loss: 68.3243\n",
            "Epoch 21/200\n",
            "283/283 [==============================] - 0s 416us/sample - loss: 91.8384 - val_loss: 64.4451\n",
            "Epoch 22/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 87.7086 - val_loss: 56.8326\n",
            "Epoch 23/200\n",
            "283/283 [==============================] - 0s 382us/sample - loss: 83.2574 - val_loss: 55.6008\n",
            "Epoch 24/200\n",
            "283/283 [==============================] - 0s 389us/sample - loss: 85.9526 - val_loss: 54.4384\n",
            "Epoch 25/200\n",
            "283/283 [==============================] - 0s 348us/sample - loss: 80.5666 - val_loss: 50.0014\n",
            "Epoch 26/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 75.9231 - val_loss: 44.9362\n",
            "Epoch 27/200\n",
            "283/283 [==============================] - 0s 354us/sample - loss: 79.8103 - val_loss: 44.6277\n",
            "Epoch 28/200\n",
            "283/283 [==============================] - 0s 334us/sample - loss: 76.7422 - val_loss: 44.4763\n",
            "Epoch 29/200\n",
            "283/283 [==============================] - 0s 376us/sample - loss: 69.4189 - val_loss: 40.7008\n",
            "Epoch 30/200\n",
            "283/283 [==============================] - 0s 345us/sample - loss: 67.8304 - val_loss: 43.7876\n",
            "Epoch 31/200\n",
            "283/283 [==============================] - 0s 371us/sample - loss: 67.5110 - val_loss: 51.7477\n",
            "Epoch 32/200\n",
            "283/283 [==============================] - 0s 340us/sample - loss: 64.1108 - val_loss: 38.4130\n",
            "Epoch 33/200\n",
            "283/283 [==============================] - 0s 348us/sample - loss: 68.0883 - val_loss: 36.4312\n",
            "Epoch 34/200\n",
            "283/283 [==============================] - 0s 356us/sample - loss: 63.9685 - val_loss: 40.2167\n",
            "Epoch 35/200\n",
            "283/283 [==============================] - 0s 324us/sample - loss: 62.6500 - val_loss: 34.8277\n",
            "Epoch 36/200\n",
            "283/283 [==============================] - 0s 345us/sample - loss: 59.7589 - val_loss: 43.4929\n",
            "Epoch 37/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 58.7829 - val_loss: 32.8933\n",
            "Epoch 38/200\n",
            "283/283 [==============================] - 0s 341us/sample - loss: 55.7686 - val_loss: 34.2295\n",
            "Epoch 39/200\n",
            "283/283 [==============================] - 0s 353us/sample - loss: 56.4221 - val_loss: 32.1803\n",
            "Epoch 40/200\n",
            "283/283 [==============================] - 0s 352us/sample - loss: 58.4686 - val_loss: 31.1440\n",
            "Epoch 41/200\n",
            "283/283 [==============================] - 0s 324us/sample - loss: 55.5593 - val_loss: 36.7048\n",
            "Epoch 42/200\n",
            "283/283 [==============================] - 0s 341us/sample - loss: 51.9504 - val_loss: 34.8720\n",
            "Epoch 43/200\n",
            "283/283 [==============================] - 0s 323us/sample - loss: 50.9884 - val_loss: 38.6031\n",
            "Epoch 44/200\n",
            "283/283 [==============================] - 0s 336us/sample - loss: 53.0766 - val_loss: 41.5769\n",
            "Epoch 45/200\n",
            "283/283 [==============================] - 0s 389us/sample - loss: 56.3237 - val_loss: 33.2182\n",
            "Epoch 46/200\n",
            "283/283 [==============================] - 0s 317us/sample - loss: 54.6712 - val_loss: 35.5248\n",
            "Epoch 47/200\n",
            "283/283 [==============================] - 0s 383us/sample - loss: 50.4620 - val_loss: 37.1929\n",
            "Epoch 48/200\n",
            "283/283 [==============================] - 0s 383us/sample - loss: 48.9104 - val_loss: 33.6673\n",
            "Epoch 49/200\n",
            "283/283 [==============================] - 0s 335us/sample - loss: 49.5276 - val_loss: 29.4425\n",
            "Epoch 50/200\n",
            "283/283 [==============================] - 0s 329us/sample - loss: 47.8937 - val_loss: 33.4709\n",
            "Epoch 51/200\n",
            "283/283 [==============================] - 0s 349us/sample - loss: 46.8237 - val_loss: 29.2262\n",
            "Epoch 52/200\n",
            "283/283 [==============================] - 0s 390us/sample - loss: 47.9146 - val_loss: 30.6345\n",
            "Epoch 53/200\n",
            "283/283 [==============================] - 0s 393us/sample - loss: 47.8848 - val_loss: 32.2824\n",
            "Epoch 54/200\n",
            "283/283 [==============================] - 0s 379us/sample - loss: 46.1663 - val_loss: 30.0399\n",
            "Epoch 55/200\n",
            "283/283 [==============================] - 0s 336us/sample - loss: 47.8652 - val_loss: 28.2649\n",
            "Epoch 56/200\n",
            "283/283 [==============================] - 0s 344us/sample - loss: 45.5148 - val_loss: 28.6176\n",
            "Epoch 57/200\n",
            "283/283 [==============================] - 0s 357us/sample - loss: 45.2037 - val_loss: 28.3726\n",
            "Epoch 58/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 49.0712 - val_loss: 27.0765\n",
            "Epoch 59/200\n",
            "283/283 [==============================] - 0s 387us/sample - loss: 43.0622 - val_loss: 29.4448\n",
            "Epoch 60/200\n",
            "283/283 [==============================] - 0s 363us/sample - loss: 41.6063 - val_loss: 29.7992\n",
            "Epoch 61/200\n",
            "283/283 [==============================] - 0s 314us/sample - loss: 41.8747 - val_loss: 28.6945\n",
            "Epoch 62/200\n",
            "283/283 [==============================] - 0s 336us/sample - loss: 45.0784 - val_loss: 26.5345\n",
            "Epoch 63/200\n",
            "283/283 [==============================] - 0s 338us/sample - loss: 42.0126 - val_loss: 28.5100\n",
            "Epoch 64/200\n",
            "283/283 [==============================] - 0s 358us/sample - loss: 42.4749 - val_loss: 28.2704\n",
            "Epoch 65/200\n",
            "283/283 [==============================] - 0s 356us/sample - loss: 39.0159 - val_loss: 25.7213\n",
            "Epoch 66/200\n",
            "283/283 [==============================] - 0s 374us/sample - loss: 42.2193 - val_loss: 25.4883\n",
            "Epoch 67/200\n",
            "283/283 [==============================] - 0s 392us/sample - loss: 40.4431 - val_loss: 26.2052\n",
            "Epoch 68/200\n",
            "283/283 [==============================] - 0s 337us/sample - loss: 41.1143 - val_loss: 25.3151\n",
            "Epoch 69/200\n",
            "283/283 [==============================] - 0s 323us/sample - loss: 39.5380 - val_loss: 39.6765\n",
            "Epoch 70/200\n",
            "283/283 [==============================] - 0s 336us/sample - loss: 39.8827 - val_loss: 25.3780\n",
            "Epoch 71/200\n",
            "283/283 [==============================] - 0s 326us/sample - loss: 42.8252 - val_loss: 25.6671\n",
            "Epoch 72/200\n",
            "283/283 [==============================] - 0s 347us/sample - loss: 41.1379 - val_loss: 29.4376\n",
            "Epoch 73/200\n",
            "283/283 [==============================] - 0s 407us/sample - loss: 44.6910 - val_loss: 23.9609\n",
            "Epoch 74/200\n",
            "283/283 [==============================] - 0s 385us/sample - loss: 39.0495 - val_loss: 26.6749\n",
            "Epoch 75/200\n",
            "283/283 [==============================] - 0s 342us/sample - loss: 40.0066 - val_loss: 29.5913\n",
            "Epoch 76/200\n",
            "283/283 [==============================] - 0s 313us/sample - loss: 40.1719 - val_loss: 24.9669\n",
            "Epoch 77/200\n",
            "283/283 [==============================] - 0s 400us/sample - loss: 47.4312 - val_loss: 24.6377\n",
            "Epoch 78/200\n",
            "283/283 [==============================] - 0s 318us/sample - loss: 39.9295 - val_loss: 32.8989\n",
            "Epoch 79/200\n",
            "283/283 [==============================] - 0s 317us/sample - loss: 53.1429 - val_loss: 23.9898\n",
            "Epoch 80/200\n",
            "283/283 [==============================] - 0s 375us/sample - loss: 41.6828 - val_loss: 28.0330\n",
            "Epoch 81/200\n",
            "283/283 [==============================] - 0s 366us/sample - loss: 37.8445 - val_loss: 48.0116\n",
            "Epoch 82/200\n",
            "283/283 [==============================] - 0s 322us/sample - loss: 47.1388 - val_loss: 35.6008\n",
            "Epoch 83/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 38.9542 - val_loss: 33.6849\n",
            "Epoch 84/200\n",
            "283/283 [==============================] - 0s 324us/sample - loss: 38.9242 - val_loss: 29.3004\n",
            "Epoch 85/200\n",
            "283/283 [==============================] - 0s 339us/sample - loss: 38.4948 - val_loss: 34.1234\n",
            "Epoch 86/200\n",
            "283/283 [==============================] - 0s 372us/sample - loss: 41.6436 - val_loss: 45.0675\n",
            "Epoch 87/200\n",
            "283/283 [==============================] - 0s 339us/sample - loss: 40.1644 - val_loss: 36.9326\n",
            "Epoch 88/200\n",
            "283/283 [==============================] - 0s 314us/sample - loss: 39.3664 - val_loss: 31.3462\n",
            "Epoch 89/200\n",
            "283/283 [==============================] - 0s 314us/sample - loss: 36.8961 - val_loss: 28.0844\n",
            "Epoch 90/200\n",
            "283/283 [==============================] - 0s 312us/sample - loss: 38.1033 - val_loss: 28.5263\n",
            "Epoch 91/200\n",
            "283/283 [==============================] - 0s 329us/sample - loss: 43.7150 - val_loss: 25.6744\n",
            "Epoch 92/200\n",
            "283/283 [==============================] - 0s 332us/sample - loss: 45.9747 - val_loss: 24.4336\n",
            "Epoch 93/200\n",
            "283/283 [==============================] - 0s 358us/sample - loss: 52.9093 - val_loss: 35.4579\n",
            "Epoch 94/200\n",
            "283/283 [==============================] - 0s 353us/sample - loss: 40.3400 - val_loss: 25.9726\n",
            "Epoch 95/200\n",
            "283/283 [==============================] - 0s 307us/sample - loss: 40.2522 - val_loss: 26.0431\n",
            "Epoch 96/200\n",
            "283/283 [==============================] - 0s 290us/sample - loss: 36.1329 - val_loss: 27.2910\n",
            "Epoch 97/200\n",
            "283/283 [==============================] - 0s 311us/sample - loss: 43.2973 - val_loss: 39.3480\n",
            "Epoch 98/200\n",
            "283/283 [==============================] - 0s 370us/sample - loss: 41.3328 - val_loss: 24.5064\n",
            "Epoch 99/200\n",
            "283/283 [==============================] - 0s 342us/sample - loss: 36.1789 - val_loss: 31.7644\n",
            "Epoch 100/200\n",
            "283/283 [==============================] - 0s 351us/sample - loss: 50.8883 - val_loss: 27.3918\n",
            "Epoch 101/200\n",
            "283/283 [==============================] - 0s 358us/sample - loss: 36.0825 - val_loss: 58.0265\n",
            "Epoch 102/200\n",
            "283/283 [==============================] - 0s 346us/sample - loss: 42.6888 - val_loss: 23.5815\n",
            "Epoch 103/200\n",
            "283/283 [==============================] - 0s 347us/sample - loss: 35.1387 - val_loss: 25.0372\n",
            "Epoch 104/200\n",
            "283/283 [==============================] - 0s 305us/sample - loss: 39.4300 - val_loss: 26.0285\n",
            "Epoch 105/200\n",
            "283/283 [==============================] - 0s 326us/sample - loss: 41.2507 - val_loss: 22.9684\n",
            "Epoch 106/200\n",
            "283/283 [==============================] - 0s 340us/sample - loss: 33.6269 - val_loss: 26.8070\n",
            "Epoch 107/200\n",
            "283/283 [==============================] - 0s 308us/sample - loss: 41.4487 - val_loss: 31.5642\n",
            "Epoch 108/200\n",
            "283/283 [==============================] - 0s 306us/sample - loss: 37.5753 - val_loss: 26.0252\n",
            "Epoch 109/200\n",
            "283/283 [==============================] - 0s 383us/sample - loss: 34.3401 - val_loss: 23.9386\n",
            "Epoch 110/200\n",
            "283/283 [==============================] - 0s 317us/sample - loss: 38.9893 - val_loss: 24.1621\n",
            "Epoch 111/200\n",
            "283/283 [==============================] - 0s 309us/sample - loss: 36.2875 - val_loss: 24.6410\n",
            "Epoch 112/200\n",
            "283/283 [==============================] - 0s 329us/sample - loss: 38.7444 - val_loss: 28.4495\n",
            "Epoch 113/200\n",
            "283/283 [==============================] - 0s 364us/sample - loss: 37.3515 - val_loss: 23.4904\n",
            "Epoch 114/200\n",
            "283/283 [==============================] - 0s 388us/sample - loss: 38.9215 - val_loss: 25.7631\n",
            "Epoch 115/200\n",
            "283/283 [==============================] - 0s 336us/sample - loss: 33.6482 - val_loss: 30.8299\n",
            "Epoch 116/200\n",
            "283/283 [==============================] - 0s 328us/sample - loss: 37.0783 - val_loss: 24.0846\n",
            "Epoch 117/200\n",
            "283/283 [==============================] - 0s 413us/sample - loss: 41.3402 - val_loss: 23.8526\n",
            "Epoch 118/200\n",
            "283/283 [==============================] - 0s 367us/sample - loss: 38.5493 - val_loss: 24.3048\n",
            "Epoch 119/200\n",
            "283/283 [==============================] - 0s 388us/sample - loss: 36.8576 - val_loss: 39.3001\n",
            "Epoch 120/200\n",
            "283/283 [==============================] - 0s 378us/sample - loss: 37.5729 - val_loss: 43.6521\n",
            "Epoch 121/200\n",
            "283/283 [==============================] - 0s 363us/sample - loss: 38.0519 - val_loss: 24.3947\n",
            "Epoch 122/200\n",
            "283/283 [==============================] - 0s 381us/sample - loss: 35.6524 - val_loss: 22.7122\n",
            "Epoch 123/200\n",
            "283/283 [==============================] - 0s 345us/sample - loss: 33.3476 - val_loss: 36.1339\n",
            "Epoch 124/200\n",
            "283/283 [==============================] - 0s 360us/sample - loss: 39.7023 - val_loss: 30.1273\n",
            "Epoch 125/200\n",
            "283/283 [==============================] - 0s 375us/sample - loss: 34.2805 - val_loss: 29.9103\n",
            "Epoch 126/200\n",
            "283/283 [==============================] - 0s 355us/sample - loss: 34.9030 - val_loss: 33.4987\n",
            "Epoch 127/200\n",
            "283/283 [==============================] - 0s 377us/sample - loss: 35.4880 - val_loss: 23.2731\n",
            "Epoch 128/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 32.7481 - val_loss: 29.1143\n",
            "Epoch 129/200\n",
            "283/283 [==============================] - 0s 360us/sample - loss: 37.3476 - val_loss: 24.5824\n",
            "Epoch 130/200\n",
            "283/283 [==============================] - 0s 375us/sample - loss: 33.5683 - val_loss: 23.1345\n",
            "Epoch 131/200\n",
            "283/283 [==============================] - 0s 356us/sample - loss: 38.5709 - val_loss: 41.4682\n",
            "Epoch 132/200\n",
            "283/283 [==============================] - 0s 402us/sample - loss: 40.6967 - val_loss: 45.1904\n",
            "Epoch 133/200\n",
            "283/283 [==============================] - 0s 390us/sample - loss: 42.5463 - val_loss: 41.8891\n",
            "Epoch 134/200\n",
            "283/283 [==============================] - 0s 363us/sample - loss: 38.0873 - val_loss: 34.9013\n",
            "Epoch 135/200\n",
            "283/283 [==============================] - 0s 342us/sample - loss: 36.2513 - val_loss: 25.2927\n",
            "Epoch 136/200\n",
            "283/283 [==============================] - 0s 340us/sample - loss: 32.3987 - val_loss: 24.9346\n",
            "Epoch 137/200\n",
            "283/283 [==============================] - 0s 382us/sample - loss: 33.9058 - val_loss: 25.5700\n",
            "Epoch 138/200\n",
            "283/283 [==============================] - 0s 401us/sample - loss: 31.7666 - val_loss: 26.9616\n",
            "Epoch 139/200\n",
            "283/283 [==============================] - 0s 356us/sample - loss: 36.8898 - val_loss: 31.3871\n",
            "Epoch 140/200\n",
            "283/283 [==============================] - 0s 322us/sample - loss: 36.2600 - val_loss: 22.3056\n",
            "Epoch 141/200\n",
            "283/283 [==============================] - 0s 318us/sample - loss: 33.2798 - val_loss: 56.3730\n",
            "Epoch 142/200\n",
            "283/283 [==============================] - 0s 331us/sample - loss: 38.4046 - val_loss: 24.7520\n",
            "Epoch 143/200\n",
            "283/283 [==============================] - 0s 350us/sample - loss: 38.0839 - val_loss: 22.6598\n",
            "Epoch 144/200\n",
            "283/283 [==============================] - 0s 458us/sample - loss: 35.0879 - val_loss: 22.9409\n",
            "Epoch 145/200\n",
            "283/283 [==============================] - 0s 335us/sample - loss: 34.6427 - val_loss: 22.3198\n",
            "Epoch 146/200\n",
            "283/283 [==============================] - 0s 311us/sample - loss: 33.1088 - val_loss: 24.4756\n",
            "Epoch 147/200\n",
            "283/283 [==============================] - 0s 437us/sample - loss: 31.7450 - val_loss: 29.4731\n",
            "Epoch 148/200\n",
            "283/283 [==============================] - 0s 374us/sample - loss: 36.7203 - val_loss: 34.4249\n",
            "Epoch 149/200\n",
            "283/283 [==============================] - 0s 328us/sample - loss: 46.7120 - val_loss: 50.8408\n",
            "Epoch 150/200\n",
            "283/283 [==============================] - 0s 296us/sample - loss: 33.2688 - val_loss: 32.6451\n",
            "Epoch 151/200\n",
            "283/283 [==============================] - 0s 332us/sample - loss: 44.5440 - val_loss: 35.2915\n",
            "Epoch 152/200\n",
            "283/283 [==============================] - 0s 347us/sample - loss: 36.5857 - val_loss: 32.0690\n",
            "Epoch 153/200\n",
            "283/283 [==============================] - 0s 357us/sample - loss: 37.9226 - val_loss: 22.3302\n",
            "Epoch 154/200\n",
            "283/283 [==============================] - 0s 313us/sample - loss: 36.2331 - val_loss: 21.8187\n",
            "Epoch 155/200\n",
            "283/283 [==============================] - 0s 321us/sample - loss: 37.3263 - val_loss: 31.2677\n",
            "Epoch 156/200\n",
            "283/283 [==============================] - 0s 337us/sample - loss: 32.5543 - val_loss: 24.6504\n",
            "Epoch 157/200\n",
            "283/283 [==============================] - 0s 315us/sample - loss: 31.1211 - val_loss: 29.7117\n",
            "Epoch 158/200\n",
            "283/283 [==============================] - 0s 300us/sample - loss: 34.0509 - val_loss: 42.0111\n",
            "Epoch 159/200\n",
            "283/283 [==============================] - 0s 346us/sample - loss: 33.6010 - val_loss: 23.0275\n",
            "Epoch 160/200\n",
            "283/283 [==============================] - 0s 303us/sample - loss: 31.9477 - val_loss: 21.0055\n",
            "Epoch 161/200\n",
            "283/283 [==============================] - 0s 326us/sample - loss: 37.3790 - val_loss: 70.6158\n",
            "Epoch 162/200\n",
            "283/283 [==============================] - 0s 340us/sample - loss: 33.7162 - val_loss: 24.6783\n",
            "Epoch 163/200\n",
            "283/283 [==============================] - 0s 306us/sample - loss: 39.6239 - val_loss: 42.0634\n",
            "Epoch 164/200\n",
            "283/283 [==============================] - 0s 333us/sample - loss: 36.4499 - val_loss: 24.4567\n",
            "Epoch 165/200\n",
            "283/283 [==============================] - 0s 357us/sample - loss: 30.5595 - val_loss: 26.8383\n",
            "Epoch 166/200\n",
            "283/283 [==============================] - 0s 371us/sample - loss: 31.8648 - val_loss: 21.5811\n",
            "Epoch 167/200\n",
            "283/283 [==============================] - 0s 312us/sample - loss: 29.8745 - val_loss: 27.4880\n",
            "Epoch 168/200\n",
            "283/283 [==============================] - 0s 330us/sample - loss: 34.6452 - val_loss: 21.6496\n",
            "Epoch 169/200\n",
            "283/283 [==============================] - 0s 312us/sample - loss: 31.2088 - val_loss: 27.0480\n",
            "Epoch 170/200\n",
            "283/283 [==============================] - 0s 319us/sample - loss: 37.7358 - val_loss: 40.5152\n",
            "Epoch 171/200\n",
            "283/283 [==============================] - 0s 356us/sample - loss: 36.0590 - val_loss: 46.2519\n",
            "Epoch 172/200\n",
            "283/283 [==============================] - 0s 314us/sample - loss: 36.0393 - val_loss: 28.7538\n",
            "Epoch 173/200\n",
            "283/283 [==============================] - 0s 320us/sample - loss: 31.8935 - val_loss: 25.3238\n",
            "Epoch 174/200\n",
            "283/283 [==============================] - 0s 293us/sample - loss: 30.7057 - val_loss: 23.9947\n",
            "Epoch 175/200\n",
            "283/283 [==============================] - 0s 303us/sample - loss: 28.9320 - val_loss: 26.1913\n",
            "Epoch 176/200\n",
            "283/283 [==============================] - 0s 280us/sample - loss: 32.1130 - val_loss: 24.2623\n",
            "Epoch 177/200\n",
            "283/283 [==============================] - 0s 343us/sample - loss: 31.5539 - val_loss: 26.4001\n",
            "Epoch 178/200\n",
            "283/283 [==============================] - 0s 308us/sample - loss: 35.7094 - val_loss: 51.6701\n",
            "Epoch 179/200\n",
            "283/283 [==============================] - 0s 298us/sample - loss: 47.9890 - val_loss: 67.4858\n",
            "Epoch 180/200\n",
            "283/283 [==============================] - 0s 350us/sample - loss: 43.4651 - val_loss: 24.1030\n",
            "Epoch 181/200\n",
            "283/283 [==============================] - 0s 374us/sample - loss: 34.4980 - val_loss: 30.0819\n",
            "Epoch 182/200\n",
            "283/283 [==============================] - 0s 321us/sample - loss: 50.0239 - val_loss: 23.3620\n",
            "Epoch 183/200\n",
            "283/283 [==============================] - 0s 332us/sample - loss: 31.1814 - val_loss: 22.6505\n",
            "Epoch 184/200\n",
            "283/283 [==============================] - 0s 326us/sample - loss: 28.5485 - val_loss: 46.8571\n",
            "Epoch 185/200\n",
            "283/283 [==============================] - 0s 293us/sample - loss: 42.7261 - val_loss: 20.9694\n",
            "Epoch 186/200\n",
            "283/283 [==============================] - 0s 323us/sample - loss: 32.0627 - val_loss: 22.4128\n",
            "Epoch 187/200\n",
            "283/283 [==============================] - 0s 295us/sample - loss: 28.6556 - val_loss: 24.8316\n",
            "Epoch 188/200\n",
            "283/283 [==============================] - 0s 315us/sample - loss: 36.0766 - val_loss: 27.6989\n",
            "Epoch 189/200\n",
            "283/283 [==============================] - 0s 311us/sample - loss: 41.4472 - val_loss: 24.4986\n",
            "Epoch 190/200\n",
            "283/283 [==============================] - 0s 283us/sample - loss: 30.9099 - val_loss: 21.9512\n",
            "Epoch 191/200\n",
            "283/283 [==============================] - 0s 310us/sample - loss: 35.8382 - val_loss: 25.5370\n",
            "Epoch 192/200\n",
            "283/283 [==============================] - 0s 379us/sample - loss: 29.3716 - val_loss: 28.5817\n",
            "Epoch 193/200\n",
            "283/283 [==============================] - 0s 300us/sample - loss: 34.8525 - val_loss: 22.5215\n",
            "Epoch 194/200\n",
            "283/283 [==============================] - 0s 341us/sample - loss: 35.1057 - val_loss: 22.9262\n",
            "Epoch 195/200\n",
            "283/283 [==============================] - 0s 321us/sample - loss: 29.3254 - val_loss: 23.5026\n",
            "Epoch 196/200\n",
            "283/283 [==============================] - 0s 289us/sample - loss: 56.4426 - val_loss: 74.2225\n",
            "Epoch 197/200\n",
            "283/283 [==============================] - 0s 318us/sample - loss: 42.0951 - val_loss: 20.6630\n",
            "Epoch 198/200\n",
            "283/283 [==============================] - 0s 293us/sample - loss: 31.7349 - val_loss: 41.0733\n",
            "Epoch 199/200\n",
            "283/283 [==============================] - 0s 290us/sample - loss: 38.8442 - val_loss: 22.0078\n",
            "Epoch 200/200\n",
            "283/283 [==============================] - 0s 315us/sample - loss: 28.7912 - val_loss: 30.8605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s73yIJiI0agQ",
        "colab_type": "text"
      },
      "source": [
        "**모델 평가 - 학습시킨 모델에 테스트 데이터로 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G62s0jay0glU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dd65587f-f267-4d7d-b2c2-9b4831291756"
      },
      "source": [
        "eval = model.evaluate(X_test, Y_test)\n",
        "print(eval)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152/152 [==============================] - 0s 82us/sample - loss: 35.6363\n",
            "35.63625757317794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ModdBKtF0zO2",
        "colab_type": "text"
      },
      "source": [
        "**주택 가격의 예측값 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYfqLDQu0cGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "43a881c7-c9f8-42a9-bf44-4578717bb05f"
      },
      "source": [
        "y_pred = model.predict(X_test) # y_pred는 2차원 배열\n",
        "y_pred = y_pred.flatten() # 2차원 배열 y_pred를 1차원 배열로\n",
        "\n",
        "# 실제값과 예측값 비교(10개만)\n",
        "for i in range(10):\n",
        "  true_val = Y_test[i]\n",
        "  pred_val = y_pred[i]\n",
        "  squared_error = (true_val - pred_val) ** 2\n",
        "  print(f'true: {true_val}, pred: {pred_val}, se: {squared_error}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true: 18.5, pred: 27.641639709472656, se: 83.56957657780731\n",
            "true: 20.6, pred: 22.791656494140625, se: 4.803358188308769\n",
            "true: 13.9, pred: 12.025256156921387, se: 3.5146644771611695\n",
            "true: 19.9, pred: 22.174758911132812, se: 5.174528103778146\n",
            "true: 23.0, pred: 24.798614501953125, se: 3.235014126636088\n",
            "true: 30.1, pred: 38.333316802978516, se: 67.78750557820834\n",
            "true: 23.1, pred: 29.745464324951172, se: 44.16219609419871\n",
            "true: 22.0, pred: 30.749282836914062, se: 76.54995016031899\n",
            "true: 11.0, pred: 12.82346248626709, se: 3.325015438823357\n",
            "true: 33.1, pred: 36.63192367553711, se: 12.474484849819554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJvYQo9d3k3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a94cc2b9-9bea-4f01-f6e0-75974386bb16"
      },
      "source": [
        "# Epochs-MSE 그래프\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "losses = history.history['loss']\n",
        "val_losses = history.history['val_loss']\n",
        "\n",
        "plt.plot(losses, label='Train MSE')\n",
        "plt.plot(val_losses, label='Test MSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGIiICAoDEREB/j+6POXkZPcx/AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEsz0dYE3izf",
        "colab_type": "text"
      },
      "source": [
        "**모델 성능 개선 및 재 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rriajUWJ--iK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "427bcf2e-3d3f-492c-ed5f-70f6105e8a3f"
      },
      "source": [
        "# X_train ~~~> Z-Score로 변환(z = (x - mean) / std)) \n",
        "# X_test 데이터는 X_train의 평균과 표준편차를 사용해 변환하고 평가 / 예측에 사용\n",
        "import numpy as np\n",
        "\n",
        "train_data = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "test_data = (X_test - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "print('X_train_z_score =', train_data)\n",
        "print('X_test_z_score =', test_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_z_score = [[-0.41899749  2.00406281 -1.3831169  ... -0.06098538  0.14001803\n",
            "  -0.99621301]\n",
            " [-0.38843865 -0.50086828 -0.56908083 ...  0.53236501  0.44696189\n",
            "  -0.91124524]\n",
            " [-0.33594927 -0.50086828 -0.2085587  ... -0.01534305  0.44696189\n",
            "  -0.93012697]\n",
            " ...\n",
            " [-0.42395649  0.33410875 -1.14849139 ... -1.61282488  0.39208333\n",
            "  -1.11759554]\n",
            " [-0.41769145  1.37783004 -1.13275431 ... -1.47589787  0.22110197\n",
            "  -1.04881211]\n",
            " [-0.4181634   1.37783004 -1.13275431 ... -1.47589787  0.37163608\n",
            "  -1.34957104]]\n",
            "X_test_z_score = [[-0.41166006 -0.50086828 -0.19282163 ... -0.28919707  0.35107131\n",
            "  -0.32726041]\n",
            " [ 0.13661642 -0.50086828  0.96456885 ...  0.80621904  0.34496064\n",
            "  -0.19239093]\n",
            " [-0.3300931  -0.50086828 -0.46035193 ...  1.17135774 -0.6515493\n",
            "   0.49004863]\n",
            " ...\n",
            " [-0.42021826 -0.50086828 -1.31301538 ...  0.12158397  0.44696189\n",
            "  -1.01779213]\n",
            " [-0.3974875  -0.50086828 -0.40026491 ...  1.12571541  0.38033204\n",
            "   0.36057393]\n",
            " [-0.38645272 -0.50086828 -0.73789675 ... -0.47176643  0.38585515\n",
            "  -0.42706382]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLLybZOWBfmL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07d421cb-825b-467f-f001-30a5a4cec7ea"
      },
      "source": [
        "# 변환된 데이터를 학습 시킬 모델 생성\n",
        "\n",
        "# Fully-connected layer 추가 - hidden_layer 2개, output_layer\n",
        "model.add(Dense(30, activation='relu', input_dim=13)) # ~> 1st_hidden_layer\n",
        "# input_dim=X_trian.shape[1]\n",
        "\n",
        "model.add(Dense(6, activation='relu')) # ~> 2nd_hidden_layer\n",
        "\n",
        "model.add(Dense(1)) # ~> output_layer\n",
        "# 활성화 함수는 '예측 집값'이라는 output이 1개여서 없어도 된다. \n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='mean_squared_error', # 분류가 아닌 수치 예측이므로 MSE\n",
        "              optimizer='adam')\n",
        "\n",
        "# Z-Score로 변환된 X_train_z_score / X_test_z_score로 다시 학습\n",
        "history = model.fit(train_data, Y_train, batch_size=10, epochs=200,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 283 samples, validate on 71 samples\n",
            "Epoch 1/200\n",
            "283/283 [==============================] - 0s 2ms/sample - loss: 588.3751 - val_loss: 545.4627\n",
            "Epoch 2/200\n",
            "283/283 [==============================] - 0s 351us/sample - loss: 545.0236 - val_loss: 481.6746\n",
            "Epoch 3/200\n",
            "283/283 [==============================] - 0s 456us/sample - loss: 454.8361 - val_loss: 349.8999\n",
            "Epoch 4/200\n",
            "283/283 [==============================] - 0s 430us/sample - loss: 288.4221 - val_loss: 150.2882\n",
            "Epoch 5/200\n",
            "283/283 [==============================] - 0s 396us/sample - loss: 114.4178 - val_loss: 56.9421\n",
            "Epoch 6/200\n",
            "283/283 [==============================] - 0s 344us/sample - loss: 68.4373 - val_loss: 42.3682\n",
            "Epoch 7/200\n",
            "283/283 [==============================] - 0s 365us/sample - loss: 52.7339 - val_loss: 32.7874\n",
            "Epoch 8/200\n",
            "283/283 [==============================] - 0s 380us/sample - loss: 43.2787 - val_loss: 25.5224\n",
            "Epoch 9/200\n",
            "283/283 [==============================] - 0s 398us/sample - loss: 36.2495 - val_loss: 21.4730\n",
            "Epoch 10/200\n",
            "283/283 [==============================] - 0s 393us/sample - loss: 31.7094 - val_loss: 17.9606\n",
            "Epoch 11/200\n",
            "283/283 [==============================] - 0s 341us/sample - loss: 28.1697 - val_loss: 15.9377\n",
            "Epoch 12/200\n",
            "283/283 [==============================] - 0s 340us/sample - loss: 26.0200 - val_loss: 14.5086\n",
            "Epoch 13/200\n",
            "283/283 [==============================] - 0s 402us/sample - loss: 23.7266 - val_loss: 13.1204\n",
            "Epoch 14/200\n",
            "283/283 [==============================] - 0s 387us/sample - loss: 22.1652 - val_loss: 12.4487\n",
            "Epoch 15/200\n",
            "283/283 [==============================] - 0s 388us/sample - loss: 20.8757 - val_loss: 11.7754\n",
            "Epoch 16/200\n",
            "283/283 [==============================] - 0s 373us/sample - loss: 19.5839 - val_loss: 11.0342\n",
            "Epoch 17/200\n",
            "283/283 [==============================] - 0s 386us/sample - loss: 17.9259 - val_loss: 11.1163\n",
            "Epoch 18/200\n",
            "283/283 [==============================] - 0s 436us/sample - loss: 17.4199 - val_loss: 10.4559\n",
            "Epoch 19/200\n",
            "283/283 [==============================] - 0s 466us/sample - loss: 16.5341 - val_loss: 10.1954\n",
            "Epoch 20/200\n",
            "283/283 [==============================] - 0s 417us/sample - loss: 15.7983 - val_loss: 9.9006\n",
            "Epoch 21/200\n",
            "283/283 [==============================] - 0s 414us/sample - loss: 15.3134 - val_loss: 9.6163\n",
            "Epoch 22/200\n",
            "283/283 [==============================] - 0s 420us/sample - loss: 15.0160 - val_loss: 9.3235\n",
            "Epoch 23/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 14.5354 - val_loss: 9.2561\n",
            "Epoch 24/200\n",
            "283/283 [==============================] - 0s 365us/sample - loss: 14.3263 - val_loss: 9.5619\n",
            "Epoch 25/200\n",
            "283/283 [==============================] - 0s 380us/sample - loss: 13.9101 - val_loss: 8.7469\n",
            "Epoch 26/200\n",
            "283/283 [==============================] - 0s 416us/sample - loss: 13.6310 - val_loss: 8.8393\n",
            "Epoch 27/200\n",
            "283/283 [==============================] - 0s 408us/sample - loss: 13.3030 - val_loss: 8.8156\n",
            "Epoch 28/200\n",
            "283/283 [==============================] - 0s 333us/sample - loss: 13.2405 - val_loss: 8.6970\n",
            "Epoch 29/200\n",
            "283/283 [==============================] - 0s 316us/sample - loss: 12.8739 - val_loss: 8.4039\n",
            "Epoch 30/200\n",
            "283/283 [==============================] - 0s 334us/sample - loss: 12.7456 - val_loss: 8.3487\n",
            "Epoch 31/200\n",
            "283/283 [==============================] - 0s 314us/sample - loss: 12.3808 - val_loss: 8.5021\n",
            "Epoch 32/200\n",
            "283/283 [==============================] - 0s 349us/sample - loss: 12.5753 - val_loss: 8.4745\n",
            "Epoch 33/200\n",
            "283/283 [==============================] - 0s 382us/sample - loss: 12.1328 - val_loss: 8.2755\n",
            "Epoch 34/200\n",
            "283/283 [==============================] - 0s 352us/sample - loss: 11.9659 - val_loss: 8.5079\n",
            "Epoch 35/200\n",
            "283/283 [==============================] - 0s 360us/sample - loss: 11.7989 - val_loss: 8.3398\n",
            "Epoch 36/200\n",
            "283/283 [==============================] - 0s 337us/sample - loss: 11.5620 - val_loss: 8.2268\n",
            "Epoch 37/200\n",
            "283/283 [==============================] - 0s 306us/sample - loss: 11.3410 - val_loss: 8.2292\n",
            "Epoch 38/200\n",
            "283/283 [==============================] - 0s 338us/sample - loss: 11.2101 - val_loss: 8.0519\n",
            "Epoch 39/200\n",
            "283/283 [==============================] - 0s 320us/sample - loss: 11.2250 - val_loss: 8.4167\n",
            "Epoch 40/200\n",
            "283/283 [==============================] - 0s 328us/sample - loss: 10.9761 - val_loss: 8.0185\n",
            "Epoch 41/200\n",
            "283/283 [==============================] - 0s 334us/sample - loss: 10.8000 - val_loss: 8.0173\n",
            "Epoch 42/200\n",
            "283/283 [==============================] - 0s 462us/sample - loss: 10.7884 - val_loss: 8.2295\n",
            "Epoch 43/200\n",
            "283/283 [==============================] - 0s 376us/sample - loss: 10.5401 - val_loss: 8.1617\n",
            "Epoch 44/200\n",
            "283/283 [==============================] - 0s 329us/sample - loss: 10.6227 - val_loss: 7.9995\n",
            "Epoch 45/200\n",
            "283/283 [==============================] - 0s 356us/sample - loss: 10.3276 - val_loss: 8.1942\n",
            "Epoch 46/200\n",
            "283/283 [==============================] - 0s 356us/sample - loss: 10.1322 - val_loss: 7.8392\n",
            "Epoch 47/200\n",
            "283/283 [==============================] - 0s 318us/sample - loss: 10.1826 - val_loss: 8.1044\n",
            "Epoch 48/200\n",
            "283/283 [==============================] - 0s 336us/sample - loss: 10.0324 - val_loss: 7.9333\n",
            "Epoch 49/200\n",
            "283/283 [==============================] - 0s 352us/sample - loss: 9.9125 - val_loss: 8.0766\n",
            "Epoch 50/200\n",
            "283/283 [==============================] - 0s 393us/sample - loss: 9.7309 - val_loss: 7.9873\n",
            "Epoch 51/200\n",
            "283/283 [==============================] - 0s 391us/sample - loss: 9.5030 - val_loss: 8.0268\n",
            "Epoch 52/200\n",
            "283/283 [==============================] - 0s 416us/sample - loss: 9.6319 - val_loss: 7.9053\n",
            "Epoch 53/200\n",
            "283/283 [==============================] - 0s 370us/sample - loss: 9.5271 - val_loss: 8.2140\n",
            "Epoch 54/200\n",
            "283/283 [==============================] - 0s 371us/sample - loss: 9.3481 - val_loss: 7.8512\n",
            "Epoch 55/200\n",
            "283/283 [==============================] - 0s 378us/sample - loss: 9.2530 - val_loss: 7.8777\n",
            "Epoch 56/200\n",
            "283/283 [==============================] - 0s 383us/sample - loss: 9.0794 - val_loss: 7.8303\n",
            "Epoch 57/200\n",
            "283/283 [==============================] - 0s 377us/sample - loss: 9.0678 - val_loss: 8.3057\n",
            "Epoch 58/200\n",
            "283/283 [==============================] - 0s 322us/sample - loss: 9.3501 - val_loss: 7.7760\n",
            "Epoch 59/200\n",
            "283/283 [==============================] - 0s 360us/sample - loss: 9.0877 - val_loss: 7.7855\n",
            "Epoch 60/200\n",
            "283/283 [==============================] - 0s 331us/sample - loss: 8.8237 - val_loss: 7.8106\n",
            "Epoch 61/200\n",
            "283/283 [==============================] - 0s 341us/sample - loss: 8.9841 - val_loss: 7.7634\n",
            "Epoch 62/200\n",
            "283/283 [==============================] - 0s 396us/sample - loss: 8.9971 - val_loss: 8.0407\n",
            "Epoch 63/200\n",
            "283/283 [==============================] - 0s 334us/sample - loss: 9.5220 - val_loss: 8.6423\n",
            "Epoch 64/200\n",
            "283/283 [==============================] - 0s 341us/sample - loss: 9.0194 - val_loss: 7.6790\n",
            "Epoch 65/200\n",
            "283/283 [==============================] - 0s 329us/sample - loss: 8.7424 - val_loss: 7.9051\n",
            "Epoch 66/200\n",
            "283/283 [==============================] - 0s 394us/sample - loss: 8.5874 - val_loss: 8.0848\n",
            "Epoch 67/200\n",
            "283/283 [==============================] - 0s 356us/sample - loss: 8.4018 - val_loss: 7.9523\n",
            "Epoch 68/200\n",
            "283/283 [==============================] - 0s 396us/sample - loss: 8.4506 - val_loss: 7.7341\n",
            "Epoch 69/200\n",
            "283/283 [==============================] - 0s 392us/sample - loss: 8.4097 - val_loss: 7.9268\n",
            "Epoch 70/200\n",
            "283/283 [==============================] - 0s 359us/sample - loss: 8.2937 - val_loss: 7.8664\n",
            "Epoch 71/200\n",
            "283/283 [==============================] - 0s 331us/sample - loss: 8.3528 - val_loss: 7.7718\n",
            "Epoch 72/200\n",
            "283/283 [==============================] - 0s 374us/sample - loss: 8.1181 - val_loss: 7.7986\n",
            "Epoch 73/200\n",
            "283/283 [==============================] - 0s 373us/sample - loss: 8.1525 - val_loss: 7.8957\n",
            "Epoch 74/200\n",
            "283/283 [==============================] - 0s 353us/sample - loss: 8.1611 - val_loss: 8.2313\n",
            "Epoch 75/200\n",
            "283/283 [==============================] - 0s 336us/sample - loss: 8.0845 - val_loss: 7.6493\n",
            "Epoch 76/200\n",
            "283/283 [==============================] - 0s 337us/sample - loss: 8.1355 - val_loss: 7.7484\n",
            "Epoch 77/200\n",
            "283/283 [==============================] - 0s 366us/sample - loss: 7.9842 - val_loss: 7.9737\n",
            "Epoch 78/200\n",
            "283/283 [==============================] - 0s 339us/sample - loss: 8.1729 - val_loss: 8.0915\n",
            "Epoch 79/200\n",
            "283/283 [==============================] - 0s 333us/sample - loss: 7.8350 - val_loss: 7.9326\n",
            "Epoch 80/200\n",
            "283/283 [==============================] - 0s 336us/sample - loss: 7.8755 - val_loss: 7.6028\n",
            "Epoch 81/200\n",
            "283/283 [==============================] - 0s 313us/sample - loss: 7.6733 - val_loss: 7.6843\n",
            "Epoch 82/200\n",
            "283/283 [==============================] - 0s 378us/sample - loss: 7.6828 - val_loss: 7.6355\n",
            "Epoch 83/200\n",
            "283/283 [==============================] - 0s 350us/sample - loss: 7.6920 - val_loss: 7.7557\n",
            "Epoch 84/200\n",
            "283/283 [==============================] - 0s 426us/sample - loss: 7.6278 - val_loss: 7.6134\n",
            "Epoch 85/200\n",
            "283/283 [==============================] - 0s 456us/sample - loss: 7.7155 - val_loss: 7.6604\n",
            "Epoch 86/200\n",
            "283/283 [==============================] - 0s 445us/sample - loss: 7.5712 - val_loss: 7.6585\n",
            "Epoch 87/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 7.5280 - val_loss: 8.0195\n",
            "Epoch 88/200\n",
            "283/283 [==============================] - 0s 374us/sample - loss: 7.4492 - val_loss: 7.4975\n",
            "Epoch 89/200\n",
            "283/283 [==============================] - 0s 376us/sample - loss: 7.3291 - val_loss: 7.4299\n",
            "Epoch 90/200\n",
            "283/283 [==============================] - 0s 340us/sample - loss: 7.3839 - val_loss: 7.3776\n",
            "Epoch 91/200\n",
            "283/283 [==============================] - 0s 386us/sample - loss: 7.5243 - val_loss: 7.9855\n",
            "Epoch 92/200\n",
            "283/283 [==============================] - 0s 342us/sample - loss: 7.3792 - val_loss: 7.6204\n",
            "Epoch 93/200\n",
            "283/283 [==============================] - 0s 352us/sample - loss: 7.2711 - val_loss: 7.5140\n",
            "Epoch 94/200\n",
            "283/283 [==============================] - 0s 445us/sample - loss: 7.0979 - val_loss: 7.7953\n",
            "Epoch 95/200\n",
            "283/283 [==============================] - 0s 386us/sample - loss: 7.1597 - val_loss: 7.5295\n",
            "Epoch 96/200\n",
            "283/283 [==============================] - 0s 331us/sample - loss: 7.2373 - val_loss: 7.5257\n",
            "Epoch 97/200\n",
            "283/283 [==============================] - 0s 352us/sample - loss: 7.3281 - val_loss: 7.2734\n",
            "Epoch 98/200\n",
            "283/283 [==============================] - 0s 345us/sample - loss: 7.0805 - val_loss: 7.4538\n",
            "Epoch 99/200\n",
            "283/283 [==============================] - 0s 408us/sample - loss: 6.8166 - val_loss: 7.4680\n",
            "Epoch 100/200\n",
            "283/283 [==============================] - 0s 343us/sample - loss: 6.9378 - val_loss: 7.5232\n",
            "Epoch 101/200\n",
            "283/283 [==============================] - 0s 362us/sample - loss: 6.8124 - val_loss: 7.3266\n",
            "Epoch 102/200\n",
            "283/283 [==============================] - 0s 347us/sample - loss: 6.8082 - val_loss: 7.5755\n",
            "Epoch 103/200\n",
            "283/283 [==============================] - 0s 428us/sample - loss: 6.8440 - val_loss: 7.3984\n",
            "Epoch 104/200\n",
            "283/283 [==============================] - 0s 387us/sample - loss: 7.0696 - val_loss: 7.3769\n",
            "Epoch 105/200\n",
            "283/283 [==============================] - 0s 355us/sample - loss: 6.9226 - val_loss: 7.4696\n",
            "Epoch 106/200\n",
            "283/283 [==============================] - 0s 420us/sample - loss: 7.0353 - val_loss: 7.4335\n",
            "Epoch 107/200\n",
            "283/283 [==============================] - 0s 368us/sample - loss: 7.1206 - val_loss: 7.2300\n",
            "Epoch 108/200\n",
            "283/283 [==============================] - 0s 349us/sample - loss: 6.7034 - val_loss: 7.3560\n",
            "Epoch 109/200\n",
            "283/283 [==============================] - 0s 340us/sample - loss: 6.5387 - val_loss: 7.3738\n",
            "Epoch 110/200\n",
            "283/283 [==============================] - 0s 382us/sample - loss: 6.8148 - val_loss: 7.2139\n",
            "Epoch 111/200\n",
            "283/283 [==============================] - 0s 328us/sample - loss: 6.6007 - val_loss: 7.1131\n",
            "Epoch 112/200\n",
            "283/283 [==============================] - 0s 326us/sample - loss: 6.4423 - val_loss: 7.3904\n",
            "Epoch 113/200\n",
            "283/283 [==============================] - 0s 395us/sample - loss: 6.4720 - val_loss: 7.4151\n",
            "Epoch 114/200\n",
            "283/283 [==============================] - 0s 353us/sample - loss: 6.5030 - val_loss: 7.3338\n",
            "Epoch 115/200\n",
            "283/283 [==============================] - 0s 323us/sample - loss: 6.3736 - val_loss: 7.2442\n",
            "Epoch 116/200\n",
            "283/283 [==============================] - 0s 427us/sample - loss: 6.4914 - val_loss: 7.1516\n",
            "Epoch 117/200\n",
            "283/283 [==============================] - 0s 372us/sample - loss: 6.3319 - val_loss: 7.2109\n",
            "Epoch 118/200\n",
            "283/283 [==============================] - 0s 402us/sample - loss: 6.5195 - val_loss: 7.2917\n",
            "Epoch 119/200\n",
            "283/283 [==============================] - 0s 384us/sample - loss: 6.2620 - val_loss: 7.3693\n",
            "Epoch 120/200\n",
            "283/283 [==============================] - 0s 396us/sample - loss: 6.9085 - val_loss: 8.2903\n",
            "Epoch 121/200\n",
            "283/283 [==============================] - 0s 321us/sample - loss: 6.3914 - val_loss: 7.1423\n",
            "Epoch 122/200\n",
            "283/283 [==============================] - 0s 345us/sample - loss: 6.1256 - val_loss: 7.5169\n",
            "Epoch 123/200\n",
            "283/283 [==============================] - 0s 392us/sample - loss: 6.1376 - val_loss: 7.0183\n",
            "Epoch 124/200\n",
            "283/283 [==============================] - 0s 349us/sample - loss: 6.1025 - val_loss: 7.2590\n",
            "Epoch 125/200\n",
            "283/283 [==============================] - 0s 362us/sample - loss: 6.2592 - val_loss: 7.1130\n",
            "Epoch 126/200\n",
            "283/283 [==============================] - 0s 352us/sample - loss: 6.3008 - val_loss: 7.1825\n",
            "Epoch 127/200\n",
            "283/283 [==============================] - 0s 377us/sample - loss: 6.0172 - val_loss: 7.1263\n",
            "Epoch 128/200\n",
            "283/283 [==============================] - 0s 353us/sample - loss: 5.9186 - val_loss: 7.2032\n",
            "Epoch 129/200\n",
            "283/283 [==============================] - 0s 376us/sample - loss: 5.9020 - val_loss: 7.1376\n",
            "Epoch 130/200\n",
            "283/283 [==============================] - 0s 401us/sample - loss: 5.8660 - val_loss: 7.1611\n",
            "Epoch 131/200\n",
            "283/283 [==============================] - 0s 335us/sample - loss: 6.1045 - val_loss: 7.1465\n",
            "Epoch 132/200\n",
            "283/283 [==============================] - 0s 320us/sample - loss: 5.8864 - val_loss: 7.1442\n",
            "Epoch 133/200\n",
            "283/283 [==============================] - 0s 365us/sample - loss: 5.8324 - val_loss: 7.1967\n",
            "Epoch 134/200\n",
            "283/283 [==============================] - 0s 384us/sample - loss: 5.9643 - val_loss: 7.4597\n",
            "Epoch 135/200\n",
            "283/283 [==============================] - 0s 364us/sample - loss: 6.2018 - val_loss: 7.5826\n",
            "Epoch 136/200\n",
            "283/283 [==============================] - 0s 371us/sample - loss: 5.8756 - val_loss: 7.5198\n",
            "Epoch 137/200\n",
            "283/283 [==============================] - 0s 331us/sample - loss: 6.1590 - val_loss: 7.5071\n",
            "Epoch 138/200\n",
            "283/283 [==============================] - 0s 342us/sample - loss: 5.7375 - val_loss: 7.2220\n",
            "Epoch 139/200\n",
            "283/283 [==============================] - 0s 334us/sample - loss: 5.7626 - val_loss: 7.2752\n",
            "Epoch 140/200\n",
            "283/283 [==============================] - 0s 386us/sample - loss: 5.8606 - val_loss: 7.4097\n",
            "Epoch 141/200\n",
            "283/283 [==============================] - 0s 483us/sample - loss: 5.7363 - val_loss: 7.2498\n",
            "Epoch 142/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 5.6285 - val_loss: 6.9273\n",
            "Epoch 143/200\n",
            "283/283 [==============================] - 0s 334us/sample - loss: 5.7918 - val_loss: 7.2170\n",
            "Epoch 144/200\n",
            "283/283 [==============================] - 0s 407us/sample - loss: 5.8136 - val_loss: 7.2716\n",
            "Epoch 145/200\n",
            "283/283 [==============================] - 0s 413us/sample - loss: 5.6236 - val_loss: 7.1834\n",
            "Epoch 146/200\n",
            "283/283 [==============================] - 0s 387us/sample - loss: 5.5681 - val_loss: 7.2441\n",
            "Epoch 147/200\n",
            "283/283 [==============================] - 0s 409us/sample - loss: 5.5574 - val_loss: 7.8886\n",
            "Epoch 148/200\n",
            "283/283 [==============================] - 0s 387us/sample - loss: 5.6470 - val_loss: 7.4665\n",
            "Epoch 149/200\n",
            "283/283 [==============================] - 0s 484us/sample - loss: 5.7536 - val_loss: 7.1528\n",
            "Epoch 150/200\n",
            "283/283 [==============================] - 0s 412us/sample - loss: 5.6056 - val_loss: 7.0610\n",
            "Epoch 151/200\n",
            "283/283 [==============================] - 0s 425us/sample - loss: 5.9383 - val_loss: 7.0933\n",
            "Epoch 152/200\n",
            "283/283 [==============================] - 0s 394us/sample - loss: 5.6618 - val_loss: 6.8672\n",
            "Epoch 153/200\n",
            "283/283 [==============================] - 0s 403us/sample - loss: 5.5206 - val_loss: 7.1694\n",
            "Epoch 154/200\n",
            "283/283 [==============================] - 0s 421us/sample - loss: 5.7073 - val_loss: 7.1283\n",
            "Epoch 155/200\n",
            "283/283 [==============================] - 0s 387us/sample - loss: 5.4031 - val_loss: 7.0806\n",
            "Epoch 156/200\n",
            "283/283 [==============================] - 0s 413us/sample - loss: 5.2968 - val_loss: 7.2181\n",
            "Epoch 157/200\n",
            "283/283 [==============================] - 0s 414us/sample - loss: 5.3355 - val_loss: 7.1755\n",
            "Epoch 158/200\n",
            "283/283 [==============================] - 0s 406us/sample - loss: 5.5430 - val_loss: 7.3542\n",
            "Epoch 159/200\n",
            "283/283 [==============================] - 0s 367us/sample - loss: 5.4404 - val_loss: 7.5548\n",
            "Epoch 160/200\n",
            "283/283 [==============================] - 0s 369us/sample - loss: 5.3613 - val_loss: 7.3298\n",
            "Epoch 161/200\n",
            "283/283 [==============================] - 0s 443us/sample - loss: 5.4932 - val_loss: 7.0610\n",
            "Epoch 162/200\n",
            "283/283 [==============================] - 0s 358us/sample - loss: 5.3808 - val_loss: 7.2456\n",
            "Epoch 163/200\n",
            "283/283 [==============================] - 0s 324us/sample - loss: 5.7259 - val_loss: 7.0155\n",
            "Epoch 164/200\n",
            "283/283 [==============================] - 0s 409us/sample - loss: 5.4056 - val_loss: 7.0365\n",
            "Epoch 165/200\n",
            "283/283 [==============================] - 0s 363us/sample - loss: 5.3728 - val_loss: 7.4544\n",
            "Epoch 166/200\n",
            "283/283 [==============================] - 0s 389us/sample - loss: 5.1998 - val_loss: 7.2509\n",
            "Epoch 167/200\n",
            "283/283 [==============================] - 0s 366us/sample - loss: 5.2115 - val_loss: 6.9614\n",
            "Epoch 168/200\n",
            "283/283 [==============================] - 0s 358us/sample - loss: 5.1918 - val_loss: 6.9942\n",
            "Epoch 169/200\n",
            "283/283 [==============================] - 0s 346us/sample - loss: 5.0735 - val_loss: 7.3562\n",
            "Epoch 170/200\n",
            "283/283 [==============================] - 0s 328us/sample - loss: 5.3566 - val_loss: 7.2967\n",
            "Epoch 171/200\n",
            "283/283 [==============================] - 0s 371us/sample - loss: 5.1536 - val_loss: 7.0554\n",
            "Epoch 172/200\n",
            "283/283 [==============================] - 0s 376us/sample - loss: 5.0236 - val_loss: 7.0587\n",
            "Epoch 173/200\n",
            "283/283 [==============================] - 0s 419us/sample - loss: 5.1841 - val_loss: 7.1360\n",
            "Epoch 174/200\n",
            "283/283 [==============================] - 0s 341us/sample - loss: 5.2303 - val_loss: 8.7037\n",
            "Epoch 175/200\n",
            "283/283 [==============================] - 0s 338us/sample - loss: 5.3658 - val_loss: 7.8234\n",
            "Epoch 176/200\n",
            "283/283 [==============================] - 0s 471us/sample - loss: 5.2986 - val_loss: 7.3742\n",
            "Epoch 177/200\n",
            "283/283 [==============================] - 0s 430us/sample - loss: 5.1574 - val_loss: 7.0824\n",
            "Epoch 178/200\n",
            "283/283 [==============================] - 0s 352us/sample - loss: 5.2422 - val_loss: 7.2162\n",
            "Epoch 179/200\n",
            "283/283 [==============================] - 0s 310us/sample - loss: 5.1281 - val_loss: 7.1324\n",
            "Epoch 180/200\n",
            "283/283 [==============================] - 0s 403us/sample - loss: 5.1248 - val_loss: 7.1436\n",
            "Epoch 181/200\n",
            "283/283 [==============================] - 0s 386us/sample - loss: 5.1156 - val_loss: 7.0391\n",
            "Epoch 182/200\n",
            "283/283 [==============================] - 0s 324us/sample - loss: 4.9020 - val_loss: 7.0966\n",
            "Epoch 183/200\n",
            "283/283 [==============================] - 0s 347us/sample - loss: 4.9596 - val_loss: 6.7981\n",
            "Epoch 184/200\n",
            "283/283 [==============================] - 0s 337us/sample - loss: 4.8651 - val_loss: 7.1688\n",
            "Epoch 185/200\n",
            "283/283 [==============================] - 0s 359us/sample - loss: 4.9812 - val_loss: 7.0587\n",
            "Epoch 186/200\n",
            "283/283 [==============================] - 0s 437us/sample - loss: 4.9401 - val_loss: 7.3789\n",
            "Epoch 187/200\n",
            "283/283 [==============================] - 0s 363us/sample - loss: 5.2894 - val_loss: 7.2474\n",
            "Epoch 188/200\n",
            "283/283 [==============================] - 0s 397us/sample - loss: 4.9028 - val_loss: 6.8846\n",
            "Epoch 189/200\n",
            "283/283 [==============================] - 0s 388us/sample - loss: 4.7448 - val_loss: 7.0881\n",
            "Epoch 190/200\n",
            "283/283 [==============================] - 0s 360us/sample - loss: 4.9460 - val_loss: 7.0186\n",
            "Epoch 191/200\n",
            "283/283 [==============================] - 0s 398us/sample - loss: 5.0308 - val_loss: 7.1311\n",
            "Epoch 192/200\n",
            "283/283 [==============================] - 0s 370us/sample - loss: 4.6905 - val_loss: 6.9482\n",
            "Epoch 193/200\n",
            "283/283 [==============================] - 0s 379us/sample - loss: 4.6644 - val_loss: 7.3972\n",
            "Epoch 194/200\n",
            "283/283 [==============================] - 0s 333us/sample - loss: 4.8581 - val_loss: 6.6104\n",
            "Epoch 195/200\n",
            "283/283 [==============================] - 0s 321us/sample - loss: 5.6299 - val_loss: 7.5611\n",
            "Epoch 196/200\n",
            "283/283 [==============================] - 0s 422us/sample - loss: 4.7980 - val_loss: 7.1628\n",
            "Epoch 197/200\n",
            "283/283 [==============================] - 0s 361us/sample - loss: 4.9604 - val_loss: 6.8628\n",
            "Epoch 198/200\n",
            "283/283 [==============================] - 0s 364us/sample - loss: 4.5960 - val_loss: 6.9782\n",
            "Epoch 199/200\n",
            "283/283 [==============================] - 0s 343us/sample - loss: 4.4930 - val_loss: 7.3010\n",
            "Epoch 200/200\n",
            "283/283 [==============================] - 0s 342us/sample - loss: 4.6559 - val_loss: 7.0278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_43YSIPCGUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "454aa457-611b-488a-b6ae-12c07993573e"
      },
      "source": [
        "# 재 학습된 모델을 재 평가\n",
        "eval = model.evaluate(test_data, Y_test)\n",
        "print(eval)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152/152 [==============================] - 0s 84us/sample - loss: 15.6928\n",
            "15.692849058853952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G4isEivCOaP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "c4971ca0-b33f-4ebb-d497-c490af9f4df0"
      },
      "source": [
        "y_pred = model.predict(test_data) # y_pred는 2차원 배열\n",
        "y_pred = y_pred.flatten() # 2차원 배열 y_pred를 1차원 배열로\n",
        "\n",
        "# 실제값과 예측값 비교(10개만)\n",
        "for i in range(10):\n",
        "  true_val = Y_test[i]\n",
        "  pred_val = y_pred[i]\n",
        "  squared_error = (true_val - pred_val) ** 2\n",
        "  print(f'true: {true_val}, pred: {pred_val}, se: {squared_error}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true: 18.5, pred: 21.63077735900879, se: 9.801766871682048\n",
            "true: 20.6, pred: 23.46271514892578, se: 8.19513802388915\n",
            "true: 13.9, pred: 12.542107582092285, se: 1.843871818611261\n",
            "true: 19.9, pred: 22.397878646850586, se: 6.239397734392122\n",
            "true: 23.0, pred: 22.913667678833008, se: 0.007453269678080687\n",
            "true: 30.1, pred: 29.780454635620117, se: 0.10210923989667299\n",
            "true: 23.1, pred: 22.400165557861328, se: 0.48976824640354805\n",
            "true: 22.0, pred: 23.621185302734375, se: 2.628241785801947\n",
            "true: 11.0, pred: 13.382953643798828, se: 5.678468068494112\n",
            "true: 33.1, pred: 33.6030158996582, se: 0.25302499530895006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mfKXUssCXGK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cf6aa2a9-cc3a-467b-f104-a227e1518140"
      },
      "source": [
        "# 개선된 모델의 Epochs-MSE 그래프\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "losses = history.history['loss']\n",
        "val_losses = history.history['val_loss']\n",
        "\n",
        "plt.plot(losses, label='Train MSE')\n",
        "plt.plot(val_losses, label='Test MSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSFFO4iIimkcBcRSaH/DwWSZ6hr5UdfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-PfDoidfZK_",
        "colab_type": "text"
      },
      "source": [
        "**K-Fold 교차 검증 <br/>**\n",
        "k-Fold Cross Validation은 Train Dataset을 균등하게 k개의 그룹(Fold)으로 나누고 (k - 1)개의 Test Fold와 1개의 Validation Fold로 지정.<br/>  그리고 나서 총 k회 검증을 하는데, 각 검증마다 Test Fold를 다르게 지정하여 성능을 측정. <br/>\n",
        "이런 식으로 k회 검증이 완료되면 \n",
        "각 Hyperparameter에 대한 검증 결과를 평균을 내어 Hyperparameters를 튜닝한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8pO_NV2fY6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# K-Fold 모델 생성\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    # fully-connected layer를 추가 - 은닉층 2개, 출력층\n",
        "    model.add(Dense(30, activation='relu', \n",
        "                    input_dim=X_train.shape[1]))  # 은닉층\n",
        "    model.add(Dense(6, activation='relu'))  # 은닉층\n",
        "    model.add(Dense(1))  # 출력층\n",
        "    # 모델 컴파일\n",
        "    model.compile(loss='mean_squared_error',  # 회귀(regression) - 수치 예측\n",
        "                optimizer='adam')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc4t04-RrQhT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9eb9092-acab-463c-9090-3c001639a286"
      },
      "source": [
        "k = 4  # k-fold cross-validation\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 200\n",
        "all_scores = []\n",
        "\n",
        "for i in range(k):\n",
        "    print(f'processing {i}-fold ...')\n",
        "    # k-fold CV에서 사용할 검증(validation) 데이터\n",
        "    val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
        "    val_targets = Y_train[i * num_val_samples : (i + 1) * num_val_samples]\n",
        "    \n",
        "    # k-fold CV에서 사용할 학습(train) 데이터:\n",
        "    # 원래 학습 데이터에서 검증 데이터를 제외한 나머지\n",
        "    part_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    part_train_targets = np.concatenate(\n",
        "        [Y_train[:i * num_val_samples],\n",
        "         Y_train[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    \n",
        "    # 모델 생성 & 컴파일\n",
        "    model = build_model()\n",
        "    \n",
        "    # 모델 학습\n",
        "    fitted = model.fit(part_train_data, part_train_targets,\n",
        "              epochs=num_epochs, verbose=0)\n",
        "    loss = fitted.history['loss']\n",
        "    all_scores.append(loss)\n",
        "\n",
        "# K-Fold 모델 성능 시각화\n",
        "all_scores = np.array(all_scores)\n",
        "print(all_scores)\n",
        "\n",
        "average_scores = all_scores.mean(axis=0)\n",
        "plt.plot(average_scores)\n",
        "plt.show()\n",
        "\n",
        "# 모델 평가\n",
        "eval = model.evaluate(test_data, Y_test)\n",
        "print(eval)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing 0-fold ...\n",
            "processing 1-fold ...\n",
            "processing 2-fold ...\n",
            "processing 3-fold ...\n",
            "[[566.46066674 556.66503562 545.01324142 531.37440869 515.18881753\n",
            "  496.83765819 476.31359381 452.12003298 425.70804475 396.28293558\n",
            "  364.54010721 330.8568877  296.23859342 262.34862937 227.30637101\n",
            "  195.03770642 165.40844669 138.56327315 117.30957616  99.28721252\n",
            "   85.74600329  74.71217553  66.42870744  59.73536562  54.44422838\n",
            "   49.57575584  45.75809789  42.47269187  39.45019452  36.85500955\n",
            "   34.76686412  32.97985302  31.55470689  30.22329439  29.08662884\n",
            "   28.10247738  27.15425282  26.38283005  25.73722716  25.16277384\n",
            "   24.59875332  24.03897982  23.61421208  23.22118408  22.81102635\n",
            "   22.5298224   22.14556234  21.8512353   21.60762337  21.32704168\n",
            "   21.07824805  20.83240082  20.63478089  20.41565929  20.19730366\n",
            "   19.98614608  19.81341517  19.61613936  19.46331774  19.31160867\n",
            "   19.08428863  18.92322213  18.77701629  18.579875    18.39631337\n",
            "   18.2417646   18.05187597  17.90399883  17.748768    17.58011819\n",
            "   17.40358685  17.27847322  17.11376023  16.94903671  16.81507932\n",
            "   16.67891725  16.52329517  16.44767799  16.28814126  16.15990033\n",
            "   16.0070711   15.84419939  15.74574052  15.60510736  15.45644658\n",
            "   15.33999795  15.17793642  15.06580131  14.88742032  14.75314225\n",
            "   14.63093176  14.4987601   14.3900583   14.25911403  14.11578523\n",
            "   14.05041808  13.94453812  13.76118089  13.68188902  13.58406787\n",
            "   13.44146571  13.33384917  13.22874971  13.13619056  13.0164992\n",
            "   12.94256078  12.81602507  12.72006741  12.61895512  12.49486459\n",
            "   12.39307615  12.28184457  12.19192602  12.10650471  12.00087777\n",
            "   11.9674052   11.83565998  11.74620228  11.70802141  11.5519676\n",
            "   11.47569356  11.40985425  11.3994311   11.26440388  11.17453266\n",
            "   11.11284641  11.05498039  10.98744605  10.89617689  10.84946362\n",
            "   10.76769897  10.68643368  10.60529966  10.56840733  10.48426324\n",
            "   10.42579388  10.33779802  10.26589057  10.19991903  10.14263089\n",
            "   10.07031239  10.02234855   9.95601211   9.89742197   9.82983308\n",
            "    9.78004346   9.72285648   9.74614961   9.61994809   9.61555505\n",
            "    9.54561178   9.4425094    9.3849458    9.406798     9.32815778\n",
            "    9.27862841   9.22005681   9.23602204   9.12849473   9.10457709\n",
            "    9.07728518   9.02496795   8.98823159   8.93358251   8.93329602\n",
            "    8.88065052   8.8622164    8.8334027    8.79664573   8.72486832\n",
            "    8.72427795   8.7088328    8.64179671   8.60408358   8.63224279\n",
            "    8.6682281    8.50892621   8.46029642   8.43787052   8.40918598\n",
            "    8.38943631   8.36349145   8.31883125   8.28065946   8.2475897\n",
            "    8.23365801   8.20832803   8.20041345   8.17336874   8.11566625\n",
            "    8.1056362    8.09138848   8.05851447   8.04926521   8.00807115\n",
            "    7.96395068   7.94110637   7.90270154   7.87640816   7.87613772]\n",
            " [617.16338331 607.91117733 598.12914947 587.10410854 575.98898751\n",
            "  563.77240816 551.16249096 537.04609393 523.0153469  507.80987664\n",
            "  491.33397749 474.54857113 455.50908781 436.71323022 417.08470005\n",
            "  395.80538527 374.64959923 353.31933594 330.95722812 308.70136508\n",
            "  287.66732421 265.95314909 245.86012371 226.30108941 206.97092937\n",
            "  189.8413425  174.00769617 159.83893339 146.8068075  135.21824095\n",
            "  125.49235604 116.08071245 108.03369668 101.19227336  94.99198449\n",
            "   89.59483309  84.76363895  80.29212797  75.97743885  72.12513016\n",
            "   68.45352804  64.96814538  61.9100131   58.74011276  56.14882235\n",
            "   53.49349345  51.1829404   48.97420843  46.9320568   45.05777046\n",
            "   43.24786776  41.65617166  40.07792262  38.81554981  37.56118631\n",
            "   36.34130666  35.28151456  34.30593402  33.46854293  32.72356901\n",
            "   31.99611622  31.30610533  30.68701642  30.10901252  29.62312747\n",
            "   29.15480796  28.74146555  28.3266467   27.98547927  27.65998556\n",
            "   27.3436982   27.03195354  26.76446708  26.45841864  26.21535604\n",
            "   25.97848628  25.73362629  25.51508393  25.30520117  25.07657301\n",
            "   24.89804368  24.68949469  24.51470876  24.31936809  24.12668495\n",
            "   23.95707693  23.82971278  23.62525903  23.46205888  23.31173009\n",
            "   23.13306363  22.97732046  22.85788834  22.70183319  22.52712575\n",
            "   22.39308514  22.22531966  22.08391714  21.95072448  21.8005041\n",
            "   21.68007769  21.51877874  21.37701596  21.25105729  21.126625\n",
            "   20.97932593  20.82233383  20.67649807  20.54589132  20.43089236\n",
            "   20.26164874  20.13247354  19.98249154  19.86353088  19.71686347\n",
            "   19.63591398  19.49000356  19.40676095  19.25517662  19.12847092\n",
            "   19.01457844  18.89131665  18.76404341  18.63685764  18.53234413\n",
            "   18.38631729  18.31284826  18.16017742  18.05106681  17.92888314\n",
            "   17.83461085  17.7302849   17.58056319  17.50736472  17.32511376\n",
            "   17.20456731  17.07335601  16.97248794  16.8445525   16.72186067\n",
            "   16.61329232  16.49463819  16.3850069   16.29690986  16.17735299\n",
            "   16.04232863  15.95535307  15.85732327  15.70813609  15.59978241\n",
            "   15.47907275  15.38174853  15.25449771  15.14776734  15.04892848\n",
            "   14.95705622  14.85533194  14.74660245  14.67975252  14.57781473\n",
            "   14.43151044  14.33033153  14.2059084   14.11809614  14.02015077\n",
            "   13.92149083  13.79828923  13.70912968  13.61762542  13.56820101\n",
            "   13.4097551   13.33085959  13.2458769   13.1420162   13.03660446\n",
            "   12.95828944  12.84221332  12.77515829  12.64908269  12.57187747\n",
            "   12.48195403  12.39197896  12.30009541  12.19654686  12.11979691\n",
            "   12.05148917  11.97624749  11.89732573  11.80463592  11.74207861\n",
            "   11.66568907  11.61811863  11.52152537  11.49789443  11.39916364\n",
            "   11.36450842  11.26965339  11.22312961  11.15698946  11.08314973]\n",
            " [605.25146852 591.57165206 577.76429875 563.66554157 548.48271512\n",
            "  532.422358   515.46009035 497.03661421 477.84871312 456.7929582\n",
            "  434.73397391 410.52007781 385.79276729 360.7976914  335.48680126\n",
            "  309.7363264  284.92573926 260.61337579 238.14131623 216.53653275\n",
            "  196.5414532  179.26048841 162.99077101 148.27667695 135.81007305\n",
            "  125.12389815 115.22714543 106.91130777  99.63556579  93.02296907\n",
            "   86.83490111  81.50144815  76.30002904  71.79363305  67.44565734\n",
            "   63.39027656  59.8324786   56.32281368  53.37791165  50.30656348\n",
            "   47.66470188  45.32049043  43.22724238  41.16267642  39.49907955\n",
            "   37.77116588  36.29174962  34.91819736  33.82457293  32.78551677\n",
            "   31.79675444  30.9482042   30.21774962  29.46625972  28.89457334\n",
            "   28.28976422  27.81497354  27.23571318  26.69380805  26.27088446\n",
            "   25.82185136  25.45024779  25.09418717  24.75444036  24.49785766\n",
            "   24.1471507   23.89443073  23.62059125  23.36755623  23.08592333\n",
            "   22.86057578  22.65665288  22.38098724  22.14567935  21.92929592\n",
            "   21.7462628   21.51920205  21.34304646  21.14525829  20.97947521\n",
            "   20.82285117  20.63768459  20.46376921  20.34004728  20.08197492\n",
            "   19.89933106  19.7657593   19.59572858  19.45043096  19.31178076\n",
            "   19.15949353  19.04729275  18.89681792  18.73655094  18.62514017\n",
            "   18.44518062  18.35643032  18.22836814  18.09177354  17.94048043\n",
            "   17.8238268   17.70346008  17.59108082  17.4315545   17.32086639\n",
            "   17.20632969  17.01985112  16.93528571  16.76843496  16.63511087\n",
            "   16.51927614  16.46353256  16.27227631  16.24620309  16.08176543\n",
            "   15.9408633   15.80520056  15.66862167  15.56183402  15.44933869\n",
            "   15.35104714  15.22978514  15.11348329  15.02952765  14.91468463\n",
            "   14.78022101  14.68347472  14.56886109  14.46484829  14.36047975\n",
            "   14.25945157  14.16683398  14.08219463  13.97340604  13.87626626\n",
            "   13.77491508  13.62230763  13.55539484  13.49358226  13.35338153\n",
            "   13.30140664  13.20204728  13.10753987  12.98945799  12.89841365\n",
            "   12.7947474   12.71540071  12.64283144  12.51005903  12.45653924\n",
            "   12.52642765  12.34991593  12.28280961  12.1512265   12.05981921\n",
            "   11.98715203  11.93436321  11.85442592  11.77693424  11.71987159\n",
            "   11.67285377  11.58136077  11.51165816  11.46103662  11.36839885\n",
            "   11.31442351  11.26474728  11.19501273  11.14543762  11.06488771\n",
            "   11.00032634  11.01147847  10.95426132  10.85428033  10.79844972\n",
            "   10.74462504  10.71300811  10.63778678  10.58010151  10.56092774\n",
            "   10.48358215  10.44497397  10.38656622  10.34373909  10.28642973\n",
            "   10.23664372  10.19896316  10.15652066  10.11302437  10.08903018\n",
            "   10.02765756   9.97132481   9.93184012   9.8908088    9.85905249\n",
            "    9.83113078   9.8309777    9.76055215   9.71082672   9.66983718]\n",
            " [605.55983419 601.24505019 597.89640315 594.91766793 591.98937483\n",
            "  588.47747321 583.96881264 578.50618933 572.13118153 564.69569707\n",
            "  556.03673651 546.39011102 535.05176653 522.53286192 508.42072956\n",
            "  492.28626131 474.16226747 454.21541303 432.37968043 408.17279489\n",
            "  382.4692314  355.15963309 327.57665603 298.86409513 268.99999816\n",
            "  241.32296179 212.72514447 187.34410881 163.79458721 142.68577059\n",
            "  124.53186471 108.48687337  95.5697119   84.69501782  75.88536795\n",
            "   68.31478595  62.29966796  57.00501585  52.94441269  49.28530864\n",
            "   46.20427546  43.58728073  40.95076038  39.0291111   37.06431567\n",
            "   35.46333284  34.17235218  32.83281002  31.88769129  30.88718797\n",
            "   30.00272606  29.24069838  28.64597524  28.15588831  27.62171431\n",
            "   27.1182512   26.69239595  26.3150246   25.9938521   25.59838813\n",
            "   25.35230519  25.07021993  24.82468881  24.6079927   24.382367\n",
            "   24.16533343  23.947898    23.73816768  23.55085148  23.38618206\n",
            "   23.19867333  23.06995976  22.87768833  22.70150319  22.54428909\n",
            "   22.39484361  22.26227704  22.10900007  21.97008232  21.82804878\n",
            "   21.69843976  21.55412338  21.45441083  21.26868974  21.14337285\n",
            "   21.01211138  20.88445987  20.75285249  20.59220233  20.47811227\n",
            "   20.33576102  20.20991007  20.10780783  19.98744296  19.86830306\n",
            "   19.75372018  19.60916901  19.5085204   19.3851995   19.29187726\n",
            "   19.15595847  19.06362559  18.95791361  18.83160286  18.73842912\n",
            "   18.57768933  18.47912721  18.38606421  18.26375403  18.15195962\n",
            "   18.04815495  17.94067688  17.8538048   17.77481611  17.63024173\n",
            "   17.55772632  17.50193706  17.42747176  17.24775565  17.1507632\n",
            "   17.0721868   16.94745566  16.84523081  16.76129755  16.63829419\n",
            "   16.55056264  16.4593357   16.36279044  16.28721491  16.19441544\n",
            "   16.08106309  16.01597667  15.91628086  15.85626356  15.73957886\n",
            "   15.64552108  15.55919852  15.47181262  15.39162365  15.281427\n",
            "   15.25067084  15.17250692  15.06787967  14.98686871  14.9105217\n",
            "   14.82031909  14.73344094  14.65886567  14.59585523  14.52553066\n",
            "   14.44698478  14.3613655   14.32323485  14.22635293  14.15651762\n",
            "   14.08708444  14.05655725  13.985737    13.92117496  13.83191955\n",
            "   13.7534549   13.68884839  13.59933614  13.53890906  13.46203658\n",
            "   13.41670709  13.30791684  13.2846436   13.20262453  13.12455954\n",
            "   13.06539306  13.00471213  12.96302199  12.98036539  12.83268519\n",
            "   12.76879726  12.74181073  12.64929074  12.57881631  12.53233061\n",
            "   12.50800313  12.44492392  12.35173322  12.30718251  12.26021173\n",
            "   12.18949301  12.1359807   12.09362751  12.04777406  12.01611782\n",
            "   11.95226707  11.90209635  11.86327593  11.80495653  11.75512942\n",
            "   11.70855116  11.67301958  11.61156694  11.58267033  11.52836095]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "152/152 [==============================] - 0s 370us/sample - loss: 15.4475\n",
            "15.447510468332391\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}